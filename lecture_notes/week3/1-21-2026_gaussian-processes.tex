\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}



\title{Math 106 -- Notes \\ Week 3: January 22, 2026}
\author{Ethan Levien}
\date{}

\begin{document}
\maketitle


\section*{Gaussian process (5.4)}
\subsection{Gaussian process as functional generalizations of multivariate normals}
Throughout this section we assume all random variables and stochastic processes have mean zero. This assumption simplifies notation and calculations and does not affect any of the conceptual conclusions.

\medskip

\noindent
\textbf{Maximum entropy principle (informal statement).}
Among all probability distributions on $\mathbb{R}^n$ with a prescribed covariance matrix $\Sigma$, the mean--zero multivariate Gaussian distribution with covariance $\Sigma$ is the unique distribution of maximal entropy (see Appendix \ref{app:entropy} for details). In this  sense it is the ``least informative’’ or ``most random’’ choice compatible with the covariance $\Sigma$.

\medskip

This maximum--entropy characterization provides a natural motivation for Gaussian processes. Suppose we wish to model a random function $X=\{X_t\}_{t\in {\mathbb R}}$ while specifying only its second--order statistics, encoded by a covariance function
\begin{equation}
K(s,t) = \mathrm{cov}(X_s,X_t), \qquad s,t\in {\mathbb R}.
\end{equation}
The guiding principle is to choose a stochastic process that is otherwise as unstructured as possible, subject only to this covariance constraint.


\medskip

\begin{definition}[Gaussian process]
A stochastic process $\{X_t\}_{t\in {\mathbb R}}$ is called a \emph{Gaussian process} with covariance function $K$ if for every finite collection of indices $t_1,\dots,t_n\in {\mathbb R}$, the random vector
\begin{equation}
(X_{t_1},\dots,X_{t_n})
\end{equation}
is multivariate normal with mean zero and covariance matrix
\begin{equation}
\Sigma_{i,j} = K(t_i,t_j).
\end{equation}
\end{definition}

From the maximum--entropy perspective, a Gaussian process is the unique stochastic process whose finite--dimensional distributions maximize entropy subject to the constraint that their covariances are prescribed by $K$. The existence and consistency of such a process follow from the Kolmogorov extension theorem.

\appendix

\section{Properties of Gaussian}
Recall some basic properties of (mean--zero) multivariate Gaussian vectors.  
If $\mathbf{X}\sim \mathrm{Normal}(0,\Sigma)$, then its moment--generating function (or equivalently its characteristic function) has the closed form
\begin{equation}
M_{\mathbf{X}}(t)
=\mathbb{E}\!\left[e^{\,t^\top \mathbf{X}}\right]
=\exp\!\left(\tfrac{1}{2} t^\top \Sigma\, t\right),
\qquad t\in\mathbb{R}^n.
\end{equation}
Equivalently, the characteristic function is
\begin{equation}
\varphi_{\mathbf{X}}(t)
=\mathbb{E}\!\left[e^{\,i t^\top \mathbf{X}}\right]
=\exp\!\left(-\tfrac{1}{2} t^\top \Sigma \, t\right).
\end{equation}

Several standard consequences follow immediately:

\begin{itemize}
    \item All mixed moments of $\mathbf{X}$ can be computed by differentiating $M_{\mathbf{X}}(t)$ at $t=0$.
    \item The covariance matrix satisfies
    \begin{equation}
    \mathrm{cov}(X_i,X_j)
    = \frac{\partial^2}{\partial t_i \partial t_j}
      \log M_{\mathbf{X}}(t)\bigg|_{t=0}
    = \Sigma_{i,j}.
    \end{equation}
    \item Any linear transformation of a Gaussian vector is again Gaussian:  
    if $\mathbf{Y}=A\mathbf{X}$, then
    \begin{equation}
    \mathbf{Y} \sim \mathrm{Normal}\!\left(0,\; A\Sigma A^\top\right).
    \end{equation}
\end{itemize}

These facts will be used repeatedly when we construct Gaussian processes and analyze their finite–dimensional distributions.

\subsection{Spectral expansion}



\section{Maximum Entropy Characterization of the Gaussian (optional)}\label{app:entropy}

A fundamental characterization of the Gaussian is the following:

\begin{theorem}[Maximum Entropy Characterization]
\label{thm:maxent}
Let $\mathcal{P}$ be the class of all absolutely continuous probability distributions $\mu$ on $\mathbb{R}^n$ with mean zero and covariance
\begin{equation}
\int_{\mathbb{R}^n} x x^\top \, d\mu(x)=\Sigma,
\end{equation}
where $\Sigma$ is a fixed positive--definite matrix.  
Among all $\mu\in\mathcal{P}$, the differential entropy
\begin{equation}
h(\mu) = -\int_{\mathbb{R}^n} f(x)\log f(x)\,dx,
\end{equation}
where $f$ is the density of $\mu$, is uniquely maximized by the Gaussian density
\begin{equation}
\phi_\Sigma(x)=\frac{1}{(2\pi)^{n/2} (\det \Sigma)^{1/2}}
\exp\!\left(-\frac{1}{2} x^\top \Sigma^{-1} x\right),
\end{equation}
and the maximal entropy equals
\begin{equation}
h(\phi_\Sigma)=\frac{1}{2}\log\!\big((2\pi e)^n \det \Sigma\big).
\end{equation}
\end{theorem}

\begin{proof}
Let $\mu$ be any distribution in $\mathcal{P}$ with density $f$, and let $\phi_\Sigma$ be the mean--zero Gaussian density with covariance $\Sigma$.  
Consider the Kullback--Leibler divergence
\begin{equation}
D(f\|\phi_\Sigma)
= \int_{\mathbb{R}^n} f(x)\log\frac{f(x)}{\phi_\Sigma(x)}\,dx.
\end{equation}
Since $D(f\|\phi_\Sigma)\ge 0$ for all $f$, and equality holds if and only if $f=\phi_\Sigma$ almost everywhere, we expand the divergence:
\begin{equation}
D(f\|\phi_\Sigma)
= \int f(x)\log f(x)\,dx - \int f(x)\log \phi_\Sigma(x)\,dx.
\end{equation}
The first term equals $-h(f)$.  
To compute the second term, write
\begin{equation}
\log \phi_\Sigma(x)
= -\frac{n}{2}\log(2\pi)
   - \frac{1}{2}\log(\det\Sigma)
   -\frac12 x^\top \Sigma^{-1}x.
\end{equation}
Taking expectations under $f$ and using that $\mu$ has covariance $\Sigma$,
\begin{equation}
\int f(x)\, x^\top\Sigma^{-1}x\, dx
= \mathrm{tr}\!\left(\Sigma^{-1}\Sigma\right)
= n.
\end{equation}
Hence
\begin{align}
\int f(x)\log\phi_\Sigma(x)\,dx
&= -\frac{n}{2}\log(2\pi)
   -\frac{1}{2}\log(\det\Sigma)
   -\frac12 n.
\end{align}

Substitute this into the expression for $D(f\|\phi_\Sigma)$:
\begin{align}
D(f\|\phi_\Sigma)
&= -h(f)
   +\frac{n}{2}\log(2\pi)
   +\frac{1}{2}\log(\det\Sigma)
   +\frac12 n.
\end{align}
Rearranging yields
\begin{equation}
h(f)
= \frac{n}{2}\log(2\pi e)
  + \frac12\log(\det\Sigma)
  - D(f\|\phi_\Sigma).
\end{equation}
Since $D(f\|\phi_\Sigma)\ge 0$, we obtain
\begin{equation}
h(f)\le \frac{1}{2}\log\!\big((2\pi e)^n \det\Sigma\big),
\end{equation}
with equality if and only if $f=\phi_\Sigma$ almost everywhere.  
Thus $\phi_\Sigma$ is the unique maximizer of entropy subject to the covariance constraint.
\end{proof}
\end{document}